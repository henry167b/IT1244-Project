{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we attempt to use a python library called textblob which is mainly used for the transformation of text data. As we have identified, tweets commonly have many spelling and grammar mistakes that needs to be rectified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Josiah\n",
      "[nltk_data]     Lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Josiah\n",
      "[nltk_data]     Lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "   labels                                               text\n",
      "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1       0  is upset that he can't update his Facebook by ...\n",
      "2       0  @Kenichan I dived many times for the ball. Man...\n",
      "3       0    my whole body feels itchy and like its on fire \n",
      "4       0  @nationwideclass no, it's not behaving at all....\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('Twitter Sentiment/dataset.csv', header=None)\n",
    "data.columns = ['labels', 'text']\n",
    "print(len(data))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob actually hs its own sentiment analysis method. So I tried my luck with it. The output of the polarity ranges from -1 to 1 and the huge number of 0s indicate that there is actually a lot of data that the model is unable to classify. Tired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(24999, 75000):\n",
    "    string = data.text[i]\n",
    "    tb = TextBlob(string)\n",
    "    prob = tb.sentiment.polarity\n",
    "    predictions.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17789"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x==0, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I intend to do would be similar to what we previously had but instead I'll just add an autocorrect and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>spelled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>awww that's a bummer you shoulda got david ca...</td>\n",
       "      <td>www that's a summer you should got david carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>i dived many times for the ball managed to sa...</td>\n",
       "      <td>i dived many times for the ball managed to sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itch and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no it's not behaving at all i'm mad why am i ...</td>\n",
       "      <td>no it's not behaving at all i'm mad why am i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1</td>\n",
       "      <td>Now need 8 followers to compleate 1000  Follow...</td>\n",
       "      <td>now need followers to compleate follow</td>\n",
       "      <td>now need followers to complete follow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1</td>\n",
       "      <td>I knew I had to explain something to my friend...</td>\n",
       "      <td>i knew i had to explain something to my friend...</td>\n",
       "      <td>i knew i had to explain something to my friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1</td>\n",
       "      <td>done tweeting..... til tomorrow..</td>\n",
       "      <td>done tweeting til tomorrow</td>\n",
       "      <td>done meeting til tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1</td>\n",
       "      <td>@cmozilo Act II set is pretty breath-taking -L...</td>\n",
       "      <td>act ii set is pretty breath taking love the r...</td>\n",
       "      <td>act ii set is pretty breath taking love the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1</td>\n",
       "      <td>if you don't have an artfire account to sell y...</td>\n",
       "      <td>if you don't have an artfire account to sell y...</td>\n",
       "      <td>if you don't have an attire account to sell yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       labels                                               text  \\\n",
       "0           0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1           0  is upset that he can't update his Facebook by ...   \n",
       "2           0  @Kenichan I dived many times for the ball. Man...   \n",
       "3           0    my whole body feels itchy and like its on fire    \n",
       "4           0  @nationwideclass no, it's not behaving at all....   \n",
       "...       ...                                                ...   \n",
       "99995       1  Now need 8 followers to compleate 1000  Follow...   \n",
       "99996       1  I knew I had to explain something to my friend...   \n",
       "99997       1                 done tweeting..... til tomorrow..    \n",
       "99998       1  @cmozilo Act II set is pretty breath-taking -L...   \n",
       "99999       1  if you don't have an artfire account to sell y...   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "0       awww that's a bummer you shoulda got david ca...   \n",
       "1      is upset that he can't update his facebook by ...   \n",
       "2       i dived many times for the ball managed to sa...   \n",
       "3        my whole body feels itchy and like its on fire    \n",
       "4       no it's not behaving at all i'm mad why am i ...   \n",
       "...                                                  ...   \n",
       "99995            now need followers to compleate follow    \n",
       "99996  i knew i had to explain something to my friend...   \n",
       "99997                        done tweeting til tomorrow    \n",
       "99998   act ii set is pretty breath taking love the r...   \n",
       "99999  if you don't have an artfire account to sell y...   \n",
       "\n",
       "                                                 spelled  \n",
       "0       www that's a summer you should got david carr...  \n",
       "1      is upset that he can't update his facebook by ...  \n",
       "2       i dived many times for the ball managed to sa...  \n",
       "3         my whole body feels itch and like its on fire   \n",
       "4       no it's not behaving at all i'm mad why am i ...  \n",
       "...                                                  ...  \n",
       "99995             now need followers to complete follow   \n",
       "99996  i knew i had to explain something to my friend...  \n",
       "99997                         done meeting til tomorrow   \n",
       "99998   act ii set is pretty breath taking love the r...  \n",
       "99999  if you don't have an attire account to sell yo...  \n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Twitter Sentiment/spelt.csv', header=0, index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^A-Za-z\\']+', ' ', text)  # Keep only letters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using autocorrect library as it runs the fastest and most autocorrect libraries would perform equally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "check = Speller(lang = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Josiah Lee\\Documents\\GitHub\\IT1244-Project\\autocorrect.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Josiah%20Lee/Documents/GitHub/IT1244-Project/autocorrect.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sample_txt \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Josiah%20Lee/Documents/GitHub/IT1244-Project/autocorrect.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mcleaned_text:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Josiah%20Lee/Documents/GitHub/IT1244-Project/autocorrect.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     sample_txt\u001b[39m.\u001b[39mappend(check(sentence))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Josiah%20Lee/Documents/GitHub/IT1244-Project/autocorrect.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mspelled\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sample_txt\n",
      "File \u001b[1;32mc:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autocorrect\\__init__.py:128\u001b[0m, in \u001b[0;36mSpeller.autocorrect_sentence\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mautocorrect_sentence\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[1;32m--> 128\u001b[0m     \u001b[39mreturn\u001b[39;00m re\u001b[39m.\u001b[39;49msub(\n\u001b[0;32m    129\u001b[0m         word_regexes[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlang],\n\u001b[0;32m    130\u001b[0m         \u001b[39mlambda\u001b[39;49;00m match: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautocorrect_word(match\u001b[39m.\u001b[39;49mgroup(\u001b[39m0\u001b[39;49m)),\n\u001b[0;32m    131\u001b[0m         sentence,\n\u001b[0;32m    132\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\re.py:209\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m    203\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49msub(repl, string, count)\n",
      "File \u001b[1;32mc:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autocorrect\\__init__.py:130\u001b[0m, in \u001b[0;36mSpeller.autocorrect_sentence.<locals>.<lambda>\u001b[1;34m(match)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mautocorrect_sentence\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[0;32m    128\u001b[0m     \u001b[39mreturn\u001b[39;00m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    129\u001b[0m         word_regexes[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang],\n\u001b[1;32m--> 130\u001b[0m         \u001b[39mlambda\u001b[39;00m match: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautocorrect_word(match\u001b[39m.\u001b[39;49mgroup(\u001b[39m0\u001b[39;49m)),\n\u001b[0;32m    131\u001b[0m         sentence,\n\u001b[0;32m    132\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autocorrect\\__init__.py:114\u001b[0m, in \u001b[0;36mSpeller.autocorrect_word\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m word \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 114\u001b[0m candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_candidates(word)\n\u001b[0;32m    116\u001b[0m \u001b[39m# in case the word is capitalized\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m word[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39misupper():\n",
      "File \u001b[1;32mc:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autocorrect\\__init__.py:104\u001b[0m, in \u001b[0;36mSpeller.get_candidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m     99\u001b[0m     candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexisting([word]) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexisting(w\u001b[39m.\u001b[39mtypos()) \u001b[39mor\u001b[39;00m [word]\n\u001b[0;32m    100\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    101\u001b[0m     candidates \u001b[39m=\u001b[39m (\n\u001b[0;32m    102\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexisting([word])\n\u001b[0;32m    103\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexisting(w\u001b[39m.\u001b[39mtypos())\n\u001b[1;32m--> 104\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexisting(w\u001b[39m.\u001b[39;49mdouble_typos())\n\u001b[0;32m    105\u001b[0m         \u001b[39mor\u001b[39;00m [word]\n\u001b[0;32m    106\u001b[0m     )\n\u001b[0;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m [(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnlp_data\u001b[39m.\u001b[39mget(c, \u001b[39m0\u001b[39m), c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m candidates]\n",
      "File \u001b[1;32mc:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autocorrect\\__init__.py:94\u001b[0m, in \u001b[0;36mSpeller.existing\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexisting\u001b[39m(\u001b[39mself\u001b[39m, words):\n\u001b[0;32m     93\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"{'the', 'teh'} => {'the'}\"\"\"\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m {word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnlp_data}\n",
      "File \u001b[1;32mc:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autocorrect\\__init__.py:94\u001b[0m, in \u001b[0;36m<setcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexisting\u001b[39m(\u001b[39mself\u001b[39m, words):\n\u001b[0;32m     93\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"{'the', 'teh'} => {'the'}\"\"\"\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m {word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnlp_data}\n",
      "File \u001b[1;32mc:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autocorrect\\typos.py:62\u001b[0m, in \u001b[0;36mWord._inserts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslices:\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphabet:\n\u001b[1;32m---> 62\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin((a, c, b))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_txt = []\n",
    "\n",
    "for sentence in data.cleaned_text:\n",
    "    sample_txt.append(check(sentence))\n",
    "\n",
    "data['spelled'] = sample_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('spelt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing of apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         www thats a summer you should got david carr ...\n",
       "1        is upset that he cant update his facebook by t...\n",
       "2         i dived many times for the ball managed to sa...\n",
       "3           my whole body feels itch and like its on fire \n",
       "4         no its not behaving at all im mad why am i he...\n",
       "                               ...                        \n",
       "99995               now need followers to complete follow \n",
       "99996    i knew i had to explain something to my friend...\n",
       "99997                           done meeting til tomorrow \n",
       "99998     act ii set is pretty breath taking love the r...\n",
       "99999    if you dont have an attire account to sell you...\n",
       "Name: apostrophe, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\'', '', text)\n",
    "    return text\n",
    "\n",
    "data['apostrophe'] = data['spelled'].apply(clean_text)\n",
    "data.apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [www, thats, a, summer, you, should, got, davi...\n",
       "1        [is, upset, that, he, cant, update, his, faceb...\n",
       "2        [i, dived, many, times, for, the, ball, manage...\n",
       "3        [my, whole, body, feels, itch, and, like, its,...\n",
       "4        [no, its, not, behaving, at, all, im, mad, why...\n",
       "                               ...                        \n",
       "99995         [now, need, followers, to, complete, follow]\n",
       "99996    [i, knew, i, had, to, explain, something, to, ...\n",
       "99997                       [done, meeting, til, tomorrow]\n",
       "99998    [act, ii, set, is, pretty, breath, taking, lov...\n",
       "99999    [if, you, dont, have, an, attire, account, to,...\n",
       "Name: tokens, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data['apostrophe'].apply(word_tokenize)\n",
    "data.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [www, that, s, a, summer, you, should, got, da...\n",
       "1        [be, upset, that, he, ca, nt, update, his, fac...\n",
       "2        [I, dive, many, time, for, the, ball, manage, ...\n",
       "3        [my, whole, body, feel, itch, and, like, its, ...\n",
       "4        [no, its, not, behave, at, all, I, m, mad, why...\n",
       "                               ...                        \n",
       "99995          [now, need, follower, to, complete, follow]\n",
       "99996    [I, know, I, have, to, explain, something, to,...\n",
       "99997                         [do, meeting, til, tomorrow]\n",
       "99998    [act, ii, set, be, pretty, breath, take, love,...\n",
       "99999    [if, you, do, nt, have, an, attire, account, t...\n",
       "Name: lemmatized_token, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_words(word_list):\n",
    "    doc = nlp(\" \".join(word_list))  # Join the stemmed tokens into a string\n",
    "    lemmatized_text = [token.lemma_ for token in doc]\n",
    "    return lemmatized_text\n",
    "\n",
    "data['lemmatized_token'] = data['tokens'].apply(lemmatize_words)\n",
    "data.lemmatized_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [www, summer, got, david, carr, third, day]\n",
       "1        [upset, ca, nt, update, facebook, texte, might...\n",
       "2        [i, dive, many, time, ball, manage, save, rest...\n",
       "3                    [whole, body, feel, itch, like, fire]\n",
       "4                      [behave, i, mad, i, i, ca, nt, see]\n",
       "                               ...                        \n",
       "99995                   [need, follower, complete, follow]\n",
       "99996    [i, know, i, explain, something, friend, say, ...\n",
       "99997                             [meeting, til, tomorrow]\n",
       "99998    [act, ii, set, pretty, breath, take, love, rea...\n",
       "99999    [nt, attire, account, sell, fun, thing, i, sug...\n",
       "Name: filtered_tokens, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data['filtered_tokens'] = data['lemmatized_token'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "data['filtered_tokens'] = data['filtered_tokens'].apply(lambda tokens: [word.lower() for word in tokens])\n",
    "\n",
    "\n",
    "data['filtered_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.filtered_tokens, data.labels, stratify = data.labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       shastri  invoke  mnras  priority  minute  stole  discount  airbender  \\\n",
      "0            0       0      0         0       0      0         0          0   \n",
      "1            0       0      0         0       0      0         0          0   \n",
      "2            0       0      0         0       0      0         0          0   \n",
      "3            0       0      0         0       0      0         0          0   \n",
      "4            0       0      0         0       0      0         0          0   \n",
      "...        ...     ...    ...       ...     ...    ...       ...        ...   \n",
      "99995        0       0      0         0       0      0         0          0   \n",
      "99996        0       0      0         0       0      0         0          0   \n",
      "99997        0       0      0         0       0      0         0          0   \n",
      "99998        0       0      0         0       0      0         0          0   \n",
      "99999        0       0      0         0       0      0         0          0   \n",
      "\n",
      "       gooooooood  nuit  ...  davids  crankiness  ice  javi  minister  \\\n",
      "0               0     0  ...       0           0    0     0         0   \n",
      "1               0     0  ...       0           0    0     0         0   \n",
      "2               0     0  ...       0           0    0     0         0   \n",
      "3               0     0  ...       0           0    0     0         0   \n",
      "4               0     0  ...       0           0    0     0         0   \n",
      "...           ...   ...  ...     ...         ...  ...   ...       ...   \n",
      "99995           0     0  ...       0           0    0     0         0   \n",
      "99996           0     0  ...       0           0    0     0         0   \n",
      "99997           0     0  ...       0           0    0     0         0   \n",
      "99998           0     0  ...       0           0    0     0         0   \n",
      "99999           0     0  ...       0           0    0     0         0   \n",
      "\n",
      "       analyse  personality  speshul  stranger  marbella  \n",
      "0            0            0        0         0         0  \n",
      "1            0            0        0         0         0  \n",
      "2            0            0        0         0         0  \n",
      "3            0            0        0         0         0  \n",
      "4            0            0        0         0         0  \n",
      "...        ...          ...      ...       ...       ...  \n",
      "99995        0            0        0         0         0  \n",
      "99996        0            0        0         0         0  \n",
      "99997        0            0        0         0         0  \n",
      "99998        0            0        0         0         0  \n",
      "99999        0            0        0         0         0  \n",
      "\n",
      "[100000 rows x 26219 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2), stop_words='english')\n",
    "sentences = [' '.join(words) for words in X_train]\n",
    "\n",
    "# Fit and transform the sentences\n",
    "one_hot_encoded = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "test = SelectKBest(score_func=chi2, k=3000)\n",
    "X_train_new=test.fit_transform(one_hot_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = sklearn.linear_model.LogisticRegressionCV(random_state = 42, cv = 10).fit(X_train_new, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = [' '.join(words) for words in X_test]\n",
    "X_test_new = vectorizer.transform(sen)\n",
    "X_test_new = test.transform(X_test_new)\n",
    "y_pred = clf.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7572\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.75     10000\n",
      "           1       0.75      0.78      0.76     10000\n",
      "\n",
      "    accuracy                           0.76     20000\n",
      "   macro avg       0.76      0.76      0.76     20000\n",
      "weighted avg       0.76      0.76      0.76     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Classification Report:\\n{classification_rep}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
