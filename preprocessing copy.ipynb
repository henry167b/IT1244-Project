{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\65977\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\65977\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\65977\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\65977\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\65977\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\65977\\anaconda3\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\65977\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\65977\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\65977\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\65977\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "   labels                                               text\n",
      "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1       0  is upset that he can't update his Facebook by ...\n",
      "2       0  @Kenichan I dived many times for the ball. Man...\n",
      "3       0    my whole body feels itchy and like its on fire \n",
      "4       0  @nationwideclass no, it's not behaving at all....\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('../IT1244-Project/Twitter Sentiment/dataset.csv', header=None)\n",
    "data.columns = ['labels', 'text']\n",
    "print(len(data))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "\n",
    "We clean the data by removing url mentions hashtags and keeping only the letters then convert to lower case\n",
    "\n",
    "Then we tokenise the text then we remove stop words and lanel the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\65977\\OneDrive\\Desktop\\IT1244-Project\\main.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstem_words\u001b[39m(word_list):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [stemmer\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m word_list]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mstemmed_tokens\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mtokens\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(stem_words)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Lemmatizating the individual words \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\65977\\OneDrive\\Desktop\\IT1244-Project\\main.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstem_words\u001b[39m(word_list):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [stemmer\u001b[39m.\u001b[39;49mstem(word) \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m word_list]\n",
      "\u001b[1;32mc:\\Users\\65977\\OneDrive\\Desktop\\IT1244-Project\\main.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstem_words\u001b[39m(word_list):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [stemmer\u001b[39m.\u001b[39;49mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m word_list]\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\nltk\\stem\\porter.py:671\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[1;34m(self, word, to_lowercase)\u001b[0m\n\u001b[0;32m    669\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step1a(stem)\n\u001b[0;32m    670\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step1b(stem)\n\u001b[1;32m--> 671\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step1c(stem)\n\u001b[0;32m    672\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step2(stem)\n\u001b[0;32m    673\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step3(stem)\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\nltk\\stem\\porter.py:422\u001b[0m, in \u001b[0;36mPorterStemmer._step1c\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moriginal_condition\u001b[39m(stem):\n\u001b[0;32m    420\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_contains_vowel(stem)\n\u001b[1;32m--> 422\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_rule_list(\n\u001b[0;32m    423\u001b[0m     word,\n\u001b[0;32m    424\u001b[0m     [\n\u001b[0;32m    425\u001b[0m         (\n\u001b[0;32m    426\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    428\u001b[0m             nltk_condition\n\u001b[0;32m    429\u001b[0m             \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode \u001b[39m==\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mNLTK_EXTENSIONS\n\u001b[0;32m    430\u001b[0m             \u001b[39melse\u001b[39;49;00m original_condition,\n\u001b[0;32m    431\u001b[0m         )\n\u001b[0;32m    432\u001b[0m     ],\n\u001b[0;32m    433\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\nltk\\stem\\porter.py:268\u001b[0m, in \u001b[0;36mPorterStemmer._apply_rule_list\u001b[1;34m(self, word, rules)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39mendswith(suffix):\n\u001b[0;32m    267\u001b[0m     stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replace_suffix(word, suffix, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mif\u001b[39;00m condition \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m condition(stem):\n\u001b[0;32m    269\u001b[0m         \u001b[39mreturn\u001b[39;00m stem \u001b[39m+\u001b[39m replacement\n\u001b[0;32m    270\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[39m# Don't try any further rules\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\nltk\\stem\\porter.py:417\u001b[0m, in \u001b[0;36mPorterStemmer._step1c.<locals>.nltk_condition\u001b[1;34m(stem)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnltk_condition\u001b[39m(stem):\n\u001b[0;32m    399\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[39m    This has been modified from the original Porter algorithm so\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39m    that y->i is only done when y is preceded by a consonant,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39m    conflate with 'spied', 'tried', 'flies' ...\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(stem) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_is_consonant(stem, \u001b[39mlen\u001b[39;49m(stem) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\nltk\\stem\\porter.py:126\u001b[0m, in \u001b[0;36mPorterStemmer._is_consonant\u001b[1;34m(self, word, i)\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool[val] \u001b[39m=\u001b[39m key\n\u001b[0;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvowels \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39me\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mo\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_is_consonant\u001b[39m(\u001b[39mself\u001b[39m, word, i):\n\u001b[0;32m    127\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns True if word[i] is a consonant, False otherwise\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \n\u001b[0;32m    129\u001b[0m \u001b[39m    A consonant is defined in the paper as follows:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39m        is not a consonant it is a vowel.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[39mif\u001b[39;00m word[i] \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvowels:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Clean and preprocess the text data\n",
    "import spacy\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^A-Za-z]+', ' ', text)  # Keep only letters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text\n",
    "data['tokens'] = data['cleaned_text'].apply(word_tokenize)\n",
    "\n",
    "# Stemming the individual words\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(word_list):\n",
    "    return [stemmer.stem(word) for word in word_list]\n",
    "\n",
    "data['stemmed_tokens'] = data['tokens'].apply(stem_words)\n",
    "\n",
    "# Lemmatizating the individual words \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_words(word_list):\n",
    "    doc = nlp(\" \".join(word_list))  # Join the stemmed tokens into a string\n",
    "    lemmatized_text = [token.lemma_ for token in doc]\n",
    "    return lemmatized_text\n",
    "\n",
    "data['lemmatized_token'] = data['stemmed_tokens'].apply(lemmatize_words)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#data['filtered_tokens'] = data['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_token</th>\n",
       "      <th>filtered_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>awww that s a bummer you shoulda got david ca...</td>\n",
       "      <td>[awww, that, s, a, bummer, you, shoulda, got, ...</td>\n",
       "      <td>[awww, that, s, a, bummer, you, shoulda, got, ...</td>\n",
       "      <td>[awww, that, s, a, bummer, you, shoulda, get, ...</td>\n",
       "      <td>[awww, bummer, shoulda, got, david, carr, thir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can t update his facebook by ...</td>\n",
       "      <td>[is, upset, that, he, can, t, update, his, fac...</td>\n",
       "      <td>[is, upset, that, he, can, t, updat, hi, faceb...</td>\n",
       "      <td>[be, upset, that, he, can, t, updat, hi, faceb...</td>\n",
       "      <td>[upset, update, facebook, texting, might, cry,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>i dived many times for the ball managed to sa...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "      <td>[i, dive, mani, time, for, the, ball, manag, t...</td>\n",
       "      <td>[I, dive, mani, time, for, the, ball, manag, t...</td>\n",
       "      <td>[dived, many, times, ball, managed, save, rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[my, whole, bodi, feel, itchi, and, like, it, ...</td>\n",
       "      <td>[my, whole, bodi, feel, itchi, and, like, it, ...</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no it s not behaving at all i m mad why am i ...</td>\n",
       "      <td>[no, it, s, not, behaving, at, all, i, m, mad,...</td>\n",
       "      <td>[no, it, s, not, behav, at, all, i, m, mad, wh...</td>\n",
       "      <td>[no, it, s, not, behav, at, all, I, m, mad, wh...</td>\n",
       "      <td>[behaving, mad, see]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                               text  \\\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1       0  is upset that he can't update his Facebook by ...   \n",
       "2       0  @Kenichan I dived many times for the ball. Man...   \n",
       "3       0    my whole body feels itchy and like its on fire    \n",
       "4       0  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0   awww that s a bummer you shoulda got david ca...   \n",
       "1  is upset that he can t update his facebook by ...   \n",
       "2   i dived many times for the ball managed to sa...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no it s not behaving at all i m mad why am i ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [awww, that, s, a, bummer, you, shoulda, got, ...   \n",
       "1  [is, upset, that, he, can, t, update, his, fac...   \n",
       "2  [i, dived, many, times, for, the, ball, manage...   \n",
       "3  [my, whole, body, feels, itchy, and, like, its...   \n",
       "4  [no, it, s, not, behaving, at, all, i, m, mad,...   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [awww, that, s, a, bummer, you, shoulda, got, ...   \n",
       "1  [is, upset, that, he, can, t, updat, hi, faceb...   \n",
       "2  [i, dive, mani, time, for, the, ball, manag, t...   \n",
       "3  [my, whole, bodi, feel, itchi, and, like, it, ...   \n",
       "4  [no, it, s, not, behav, at, all, i, m, mad, wh...   \n",
       "\n",
       "                                    lemmatized_token  \\\n",
       "0  [awww, that, s, a, bummer, you, shoulda, get, ...   \n",
       "1  [be, upset, that, he, can, t, updat, hi, faceb...   \n",
       "2  [I, dive, mani, time, for, the, ball, manag, t...   \n",
       "3  [my, whole, bodi, feel, itchi, and, like, it, ...   \n",
       "4  [no, it, s, not, behav, at, all, I, m, mad, wh...   \n",
       "\n",
       "                                     filtered_tokens  \n",
       "0  [awww, bummer, shoulda, got, david, carr, thir...  \n",
       "1  [upset, update, facebook, texting, might, cry,...  \n",
       "2  [dived, many, times, ball, managed, save, rest...  \n",
       "3            [whole, body, feels, itchy, like, fire]  \n",
       "4                               [behaving, mad, see]  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data['filtered_tokens'] = data['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      lucu  ta  baltimore  worry  rp  pour  grog  esteem  algo  marriages  \\\n",
      "0        0   0          0      0   0     0     0       0     0          0   \n",
      "1        0   0          0      0   0     0     0       0     0          0   \n",
      "2        0   0          0      0   0     0     0       0     0          0   \n",
      "3        0   0          0      0   0     0     0       0     0          0   \n",
      "4        0   0          0      0   0     0     0       0     0          0   \n",
      "...    ...  ..        ...    ...  ..   ...   ...     ...   ...        ...   \n",
      "9995     0   0          0      0   0     0     0       0     0          0   \n",
      "9996     0   0          0      0   0     0     0       0     0          0   \n",
      "9997     0   0          0      0   0     0     0       0     0          0   \n",
      "9998     0   0          0      0   0     0     0       0     0          0   \n",
      "9999     0   0          0      0   0     0     0       0     0          0   \n",
      "\n",
      "      ...  pace  flashden  chars  chastised  tourney  wasted  disappointed  \\\n",
      "0     ...     0         0      0          0        0       0             0   \n",
      "1     ...     0         0      0          0        0       0             0   \n",
      "2     ...     0         0      0          0        0       0             0   \n",
      "3     ...     0         0      0          0        0       0             0   \n",
      "4     ...     0         0      0          0        0       0             0   \n",
      "...   ...   ...       ...    ...        ...      ...     ...           ...   \n",
      "9995  ...     0         0      0          0        0       0             0   \n",
      "9996  ...     0         0      0          0        0       0             0   \n",
      "9997  ...     0         0      0          0        0       0             0   \n",
      "9998  ...     0         0      0          0        0       0             0   \n",
      "9999  ...     0         0      0          0        0       0             0   \n",
      "\n",
      "      sooooooooooooo  canceled  abs  \n",
      "0                  0         0    0  \n",
      "1                  0         0    0  \n",
      "2                  0         0    0  \n",
      "3                  0         0    0  \n",
      "4                  0         0    0  \n",
      "...              ...       ...  ...  \n",
      "9995               0         0    0  \n",
      "9996               0         0    0  \n",
      "9997               0         0    0  \n",
      "9998               0         0    0  \n",
      "9999               0         0    0  \n",
      "\n",
      "[10000 rows x 13105 columns]\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding\n",
    "categories = list(set([item for sublist in data['filtered_tokens'] for item in sublist]))\n",
    "vectorizer = CountVectorizer(binary=True, vocabulary=categories)\n",
    "sentences = [' '.join(words) for words in data['filtered_tokens']]\n",
    "\n",
    "# Fit and transform the sentences\n",
    "one_hot_encoded = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Convert the one-hot encoded data to a DataFrame\n",
    "encoded_df = pd.DataFrame(one_hot_encoded.toarray(), columns = categories)\n",
    "#encoded_df['labels'] = data['labels']\n",
    "\n",
    "# use the print function to check if the one hot encoding is correct\n",
    "#print(encoded_df.iloc[954,:].sum(),sentences[954]) \n",
    "print(encoded_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "count = (data['labels'] == 0).sum()\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(encoded_df, data['labels'], stratify=data['labels'], test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858     0\n",
      "96842    1\n",
      "3905     0\n",
      "97684    1\n",
      "1564     0\n",
      "        ..\n",
      "99987    1\n",
      "1717     0\n",
      "98795    1\n",
      "3691     0\n",
      "131      0\n",
      "Name: labels, Length: 8000, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train)\n",
    "sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73      1000\n",
      "           1       0.73      0.74      0.73      1000\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.73      0.73      0.73      2000\n",
      "weighted avg       0.73      0.73      0.73      2000\n",
      "\n",
      "['36.05%', '13.95%', '13.00%', '37.00%']\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix  # Import the confusion_matrix function\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "#X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def model_Evaluate(model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(Y_test, y_pred)) \n",
    "    return y_pred \n",
    "#Precision: ratio of true positives to the total number of predicted positives\n",
    "#Recall: ratio of true positives to the total number of actual positives\n",
    "#F1-Score: mean of precision and recall\n",
    "#Support: number of occurrences of each class in the true labels (y_test).\n",
    "\n",
    "\n",
    "#confuion matrix, TP,TN, FP, FN\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, y_train)\n",
    "y_pred = BNB.predict(X_test)\n",
    "cf_matrix = confusion_matrix(y_test, y_pred) \n",
    "#TP,FN, FP, TN in %%%% \n",
    "group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "print(classification_report(y_test, y_pred)) \n",
    "print(group_percentages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we do bigram analysos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an N-gram model (e.g., bigrams)\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_ngrams = ngram_vectorizer.fit_transform(X_train)\n",
    "X_test_ngrams = ngram_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use a multinomial nb classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2791, 800]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\65977\\OneDrive\\Desktop\\IT1244-Project\\main.ipynb Cell 18\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Train a classifier (e.g., Multinomial Naive Bayes)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m classifier \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(X_train_ngrams, Y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/65977/OneDrive/Desktop/IT1244-Project/main.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m y_pred \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mpredict(X_test_ngrams)\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:749\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \n\u001b[0;32m    731\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 749\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X_y(X, y)\n\u001b[0;32m    750\u001b[0m _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m    752\u001b[0m labelbin \u001b[39m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:583\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[1;34m(self, X, y, reset)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_X_y\u001b[39m(\u001b[39mself\u001b[39m, X, y, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    582\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49mreset)\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m-> 1124\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1126\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\65977\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2791, 800]"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train a classifier (e.g., Multinomial Naive Bayes)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_ngrams, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test_ngrams)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "classification_rep = classification_report(Y_test, y_pred, target_names=le.classes_)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Classification Report:\\n{classification_rep}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
