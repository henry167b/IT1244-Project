{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textBlob in c:\\users\\josiah lee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\josiah lee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textBlob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\josiah lee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textBlob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\josiah lee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textBlob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\josiah lee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textBlob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\josiah lee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textBlob) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\josiah lee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk>=3.1->textBlob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install textBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we attempt to use a python library called textblob which is mainly used for the transformation of text data. As we have identified, tweets commonly have many spelling and grammar mistakes that needs to be rectified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Josiah\n",
      "[nltk_data]     Lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Josiah\n",
      "[nltk_data]     Lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "   labels                                               text\n",
      "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1       0  is upset that he can't update his Facebook by ...\n",
      "2       0  @Kenichan I dived many times for the ball. Man...\n",
      "3       0    my whole body feels itchy and like its on fire \n",
      "4       0  @nationwideclass no, it's not behaving at all....\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('Twitter Sentiment/dataset.csv', header=None)\n",
    "data.columns = ['labels', 'text']\n",
    "print(len(data))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob actually hs its own sentiment analysis method. So I tried my luck with it. The output of the polarity ranges from -1 to 1 and the huge number of 0s indicate that there is actually a lot of data that the model is unable to classify. Tired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(24999, 75000):\n",
    "    string = data.text[i]\n",
    "    tb = TextBlob(string)\n",
    "    prob = tb.sentiment.polarity\n",
    "    predictions.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17789"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x==0, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I intend to do would be similar to what we previously had but instead I'll just add an autocorrect and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>spelled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>awww that's a bummer you shoulda got david ca...</td>\n",
       "      <td>www that's a summer you should got david carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>i dived many times for the ball managed to sa...</td>\n",
       "      <td>i dived many times for the ball managed to sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itch and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no it's not behaving at all i'm mad why am i ...</td>\n",
       "      <td>no it's not behaving at all i'm mad why am i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1</td>\n",
       "      <td>Now need 8 followers to compleate 1000  Follow...</td>\n",
       "      <td>now need followers to compleate follow</td>\n",
       "      <td>now need followers to complete follow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1</td>\n",
       "      <td>I knew I had to explain something to my friend...</td>\n",
       "      <td>i knew i had to explain something to my friend...</td>\n",
       "      <td>i knew i had to explain something to my friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1</td>\n",
       "      <td>done tweeting..... til tomorrow..</td>\n",
       "      <td>done tweeting til tomorrow</td>\n",
       "      <td>done meeting til tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1</td>\n",
       "      <td>@cmozilo Act II set is pretty breath-taking -L...</td>\n",
       "      <td>act ii set is pretty breath taking love the r...</td>\n",
       "      <td>act ii set is pretty breath taking love the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1</td>\n",
       "      <td>if you don't have an artfire account to sell y...</td>\n",
       "      <td>if you don't have an artfire account to sell y...</td>\n",
       "      <td>if you don't have an attire account to sell yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       labels                                               text  \\\n",
       "0           0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1           0  is upset that he can't update his Facebook by ...   \n",
       "2           0  @Kenichan I dived many times for the ball. Man...   \n",
       "3           0    my whole body feels itchy and like its on fire    \n",
       "4           0  @nationwideclass no, it's not behaving at all....   \n",
       "...       ...                                                ...   \n",
       "99995       1  Now need 8 followers to compleate 1000  Follow...   \n",
       "99996       1  I knew I had to explain something to my friend...   \n",
       "99997       1                 done tweeting..... til tomorrow..    \n",
       "99998       1  @cmozilo Act II set is pretty breath-taking -L...   \n",
       "99999       1  if you don't have an artfire account to sell y...   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "0       awww that's a bummer you shoulda got david ca...   \n",
       "1      is upset that he can't update his facebook by ...   \n",
       "2       i dived many times for the ball managed to sa...   \n",
       "3        my whole body feels itchy and like its on fire    \n",
       "4       no it's not behaving at all i'm mad why am i ...   \n",
       "...                                                  ...   \n",
       "99995            now need followers to compleate follow    \n",
       "99996  i knew i had to explain something to my friend...   \n",
       "99997                        done tweeting til tomorrow    \n",
       "99998   act ii set is pretty breath taking love the r...   \n",
       "99999  if you don't have an artfire account to sell y...   \n",
       "\n",
       "                                                 spelled  \n",
       "0       www that's a summer you should got david carr...  \n",
       "1      is upset that he can't update his facebook by ...  \n",
       "2       i dived many times for the ball managed to sa...  \n",
       "3         my whole body feels itch and like its on fire   \n",
       "4       no it's not behaving at all i'm mad why am i ...  \n",
       "...                                                  ...  \n",
       "99995             now need followers to complete follow   \n",
       "99996  i knew i had to explain something to my friend...  \n",
       "99997                         done meeting til tomorrow   \n",
       "99998   act ii set is pretty breath taking love the r...  \n",
       "99999  if you don't have an attire account to sell yo...  \n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('spelt.csv', header=0, index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^A-Za-z\\']+', ' ', text)  # Keep only letters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using autocorrect library as it runs the fastest and most autocorrect libraries would perform equally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "check = Speller(lang = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = []\n",
    "\n",
    "for sentence in data.cleaned_text:\n",
    "    sample_txt.append(check(sentence))\n",
    "\n",
    "data['spelled'] = sample_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('spelt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing of apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         www thats a summer you should got david carr ...\n",
       "1        is upset that he cant update his facebook by t...\n",
       "2         i dived many times for the ball managed to sa...\n",
       "3           my whole body feels itch and like its on fire \n",
       "4         no its not behaving at all im mad why am i he...\n",
       "                               ...                        \n",
       "99995               now need followers to complete follow \n",
       "99996    i knew i had to explain something to my friend...\n",
       "99997                           done meeting til tomorrow \n",
       "99998     act ii set is pretty breath taking love the r...\n",
       "99999    if you dont have an attire account to sell you...\n",
       "Name: apostrophe, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\'', '', text)\n",
    "    return text\n",
    "\n",
    "data['apostrophe'] = data['spelled'].apply(clean_text)\n",
    "data.apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [www, thats, a, summer, you, should, got, davi...\n",
       "1        [is, upset, that, he, cant, update, his, faceb...\n",
       "2        [i, dived, many, times, for, the, ball, manage...\n",
       "3        [my, whole, body, feels, itch, and, like, its,...\n",
       "4        [no, its, not, behaving, at, all, im, mad, why...\n",
       "                               ...                        \n",
       "99995         [now, need, followers, to, complete, follow]\n",
       "99996    [i, knew, i, had, to, explain, something, to, ...\n",
       "99997                       [done, meeting, til, tomorrow]\n",
       "99998    [act, ii, set, is, pretty, breath, taking, lov...\n",
       "99999    [if, you, dont, have, an, attire, account, to,...\n",
       "Name: tokens, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data['apostrophe'].apply(word_tokenize)\n",
    "data.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [www, that, s, a, summer, you, should, got, da...\n",
       "1        [be, upset, that, he, ca, nt, update, his, fac...\n",
       "2        [I, dive, many, time, for, the, ball, manage, ...\n",
       "3        [my, whole, body, feel, itch, and, like, its, ...\n",
       "4        [no, its, not, behave, at, all, I, m, mad, why...\n",
       "                               ...                        \n",
       "99995          [now, need, follower, to, complete, follow]\n",
       "99996    [I, know, I, have, to, explain, something, to,...\n",
       "99997                         [do, meeting, til, tomorrow]\n",
       "99998    [act, ii, set, be, pretty, breath, take, love,...\n",
       "99999    [if, you, do, nt, have, an, attire, account, t...\n",
       "Name: lemmatized_token, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_words(word_list):\n",
    "    doc = nlp(\" \".join(word_list))  # Join the stemmed tokens into a string\n",
    "    lemmatized_text = [token.lemma_ for token in doc]\n",
    "    return lemmatized_text\n",
    "\n",
    "data['lemmatized_token'] = data['tokens'].apply(lemmatize_words)\n",
    "data.lemmatized_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [www, summer, got, david, carr, third, day]\n",
       "1        [upset, ca, nt, update, facebook, texte, might...\n",
       "2        [i, dive, many, time, ball, manage, save, rest...\n",
       "3                    [whole, body, feel, itch, like, fire]\n",
       "4                      [behave, i, mad, i, i, ca, nt, see]\n",
       "                               ...                        \n",
       "99995                   [need, follower, complete, follow]\n",
       "99996    [i, know, i, explain, something, friend, say, ...\n",
       "99997                             [meeting, til, tomorrow]\n",
       "99998    [act, ii, set, pretty, breath, take, love, rea...\n",
       "99999    [nt, attire, account, sell, fun, thing, i, sug...\n",
       "Name: filtered_tokens, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data['filtered_tokens'] = data['lemmatized_token'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "data['filtered_tokens'] = data['filtered_tokens'].apply(lambda tokens: [word.lower() for word in tokens])\n",
    "\n",
    "\n",
    "data['filtered_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       shastri  invoke  mnras  priority  minute  stole  discount  airbender  \\\n",
      "0            0       0      0         0       0      0         0          0   \n",
      "1            0       0      0         0       0      0         0          0   \n",
      "2            0       0      0         0       0      0         0          0   \n",
      "3            0       0      0         0       0      0         0          0   \n",
      "4            0       0      0         0       0      0         0          0   \n",
      "...        ...     ...    ...       ...     ...    ...       ...        ...   \n",
      "99995        0       0      0         0       0      0         0          0   \n",
      "99996        0       0      0         0       0      0         0          0   \n",
      "99997        0       0      0         0       0      0         0          0   \n",
      "99998        0       0      0         0       0      0         0          0   \n",
      "99999        0       0      0         0       0      0         0          0   \n",
      "\n",
      "       gooooooood  nuit  ...  davids  crankiness  ice  javi  minister  \\\n",
      "0               0     0  ...       0           0    0     0         0   \n",
      "1               0     0  ...       0           0    0     0         0   \n",
      "2               0     0  ...       0           0    0     0         0   \n",
      "3               0     0  ...       0           0    0     0         0   \n",
      "4               0     0  ...       0           0    0     0         0   \n",
      "...           ...   ...  ...     ...         ...  ...   ...       ...   \n",
      "99995           0     0  ...       0           0    0     0         0   \n",
      "99996           0     0  ...       0           0    0     0         0   \n",
      "99997           0     0  ...       0           0    0     0         0   \n",
      "99998           0     0  ...       0           0    0     0         0   \n",
      "99999           0     0  ...       0           0    0     0         0   \n",
      "\n",
      "       analyse  personality  speshul  stranger  marbella  \n",
      "0            0            0        0         0         0  \n",
      "1            0            0        0         0         0  \n",
      "2            0            0        0         0         0  \n",
      "3            0            0        0         0         0  \n",
      "4            0            0        0         0         0  \n",
      "...        ...          ...      ...       ...       ...  \n",
      "99995        0            0        0         0         0  \n",
      "99996        0            0        0         0         0  \n",
      "99997        0            0        0         0         0  \n",
      "99998        0            0        0         0         0  \n",
      "99999        0            0        0         0         0  \n",
      "\n",
      "[100000 rows x 26219 columns]\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding\n",
    "categories = list(set([item for sublist in data.filtered_tokens for item in sublist]))\n",
    "#print(categories)\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, vocabulary = categories, lowercase=True)\n",
    "sentences = [' '.join(words) for words in data.filtered_tokens]\n",
    "\n",
    "# Fit and transform the sentences\n",
    "one_hot_encoded = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Convert the one-hot encoded data to a DataFrame\n",
    "encoded_df = pd.DataFrame(one_hot_encoded.toarray(), columns = categories)\n",
    "\n",
    "# use the print function to check if the one hot encoding is correct\n",
    "#print(encoded_df.iloc[954,:].sum(),sentences[954]) \n",
    "print(encoded_df)\n",
    "#print(categories)\n",
    "\n",
    "encoded = encoded_df.iloc[40000:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(encoded_df, data.labels, stratify=data.labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41429    0\n",
      "43292    0\n",
      "48382    0\n",
      "42471    0\n",
      "40665    0\n",
      "        ..\n",
      "42610    0\n",
      "43277    0\n",
      "43700    0\n",
      "44457    0\n",
      "48180    0\n",
      "Name: labels, Length: 8000, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "99995    1\n",
       "99996    1\n",
       "99997    1\n",
       "99998    1\n",
       "99999    1\n",
       "Name: labels, Length: 100000, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train)\n",
    "len(y_train)\n",
    "data.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "test = SelectKBest(score_func=chi2, k=3000)\n",
    "X_new=test.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Josiah Lee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = sklearn.linear_model.LogisticRegressionCV(random_state = 42, cv = 10).fit(X_new, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = test.transform(X_test)\n",
    "y_pred = clf.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7572\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.75     10000\n",
      "           1       0.75      0.78      0.76     10000\n",
      "\n",
      "    accuracy                           0.76     20000\n",
      "   macro avg       0.76      0.76      0.76     20000\n",
      "weighted avg       0.76      0.76      0.76     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Classification Report:\\n{classification_rep}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
